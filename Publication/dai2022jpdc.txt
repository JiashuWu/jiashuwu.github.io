@article{DAI2022108,
title = {Towards scalable and efficient Deep-RL in edge computing: A game-based partition approach},
journal = {Journal of Parallel and Distributed Computing},
volume = {168},
pages = {108-119},
year = {2022},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2022.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0743731522001411},
author = {Hao Dai and Jiashu Wu and Yang Wang and Chengzhong Xu},
keywords = {Mobile edge computing, Deep reinforcement learning, DNN partition, Game theory},
abstract = {Currently, most edge-based Deep Reinforcement Learning (Deep-RL) applications have been deployed in the edge network, however, their mainstream studies are still short of adequate considerations on its limited compute and bandwidth resources. In this paper, we investigate the near on-policy of actions taking in distributed Deep-RL architecture, and propose a “hybrid near on-policy” Deep-RL framework, called Coknight, by leveraging a game-theory based DNN partition approach. We first formulate the partition problem into a variant of knapsack problem in device-edge setting, and then transform it into a potential game with a formal proof. Finally, we show the problem is NP-complete whereby an efficient distributed algorithm based on the potential game theory is developed from device perspective to achieve fast and dynamic partitioning. Coknight not only significantly improves the resource efficiency of the Deep-RL but also allows the inference to enforce the scalability of the actor policy. We prototype the framework with extensive experiments to validate our findings. The experimental results show that with the premise of a rapid convergence guarantee, Coknight, compared with Seed-RL, can reduce GPU utilization by 30% while providing large-scale scalability.}
}